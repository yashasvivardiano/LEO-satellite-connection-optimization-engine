{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dual AI Training: Predictive Steering and Anomaly Detection\n",
        "\n",
        "This notebook trains two models on the hybrid wired/satellite datasets:\n",
        "\n",
        "- Predictive model (supervised): predicts `current_optimal_path` 60s ahead.\n",
        "- Anomaly detector (unsupervised): LSTM Autoencoder trained on normal data to detect anomalies via reconstruction error.\n",
        "\n",
        "It is optimized to run on Kaggle (GPU optional). Models and artifacts are saved under `/kaggle/working/`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment and imports\n",
        "%pip -q install \"numpy==1.26.4\" \"pandas==2.2.2\" \"scikit-learn==1.4.2\" \"tensorflow==2.15.0\" \"seaborn==0.13.2\" \"matplotlib==3.8.4\" \"pyarrow==14.0.2\" \"fastparquet==2024.2.0\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "# Reduce verbose TensorFlow/CUDA logs\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "\n",
        "# GPU availability & memory growth\n",
        "physical_gpus = tf.config.list_physical_devices('GPU')\n",
        "if physical_gpus:\n",
        "    try:\n",
        "        for gpu in physical_gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"GPUs detected: {len(physical_gpus)} -> using {physical_gpus[0].name}\")\n",
        "    except Exception as e:\n",
        "        print(\"GPU memory growth setup failed:\", e)\n",
        "else:\n",
        "    print(\"No GPU detected; running on CPU\")\n",
        "\n",
        "print(tf.__version__)\n",
        "ARTIFACTS_DIR = Path('/kaggle/working')\n",
        "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths to Kaggle inputs (update if your dataset slug differs)\n",
        "INPUT_DIR = Path('/kaggle/input/satellite-and-wired')\n",
        "DATASET_A = INPUT_DIR / 'hybrid_v1_dataset_a.csv'\n",
        "DATASET_B = INPUT_DIR / 'hybrid_v1_dataset_b.csv'\n",
        "assert DATASET_A.exists(), DATASET_A\n",
        "assert DATASET_B.exists(), DATASET_B\n",
        "\n",
        "print('Dataset A:', DATASET_A)\n",
        "print('Dataset B:', DATASET_B)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility: windowing and splits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def create_sequences(df: pd.DataFrame, feature_cols, label_col=None, window: int = 12, horizon: int = 6):\n",
        "    \"\"\"\n",
        "    Create overlapping windowed sequences of length `window`.\n",
        "    If label_col is provided, it creates classification targets at t+`horizon`.\n",
        "    Assumes rows are chronological.\n",
        "    \"\"\"\n",
        "    X = df[feature_cols].values\n",
        "    y = None\n",
        "    if label_col is not None:\n",
        "        y_raw = df[label_col].values\n",
        "    seq_X, seq_y = [], []\n",
        "    for i in range(len(df) - window - horizon + 1):\n",
        "        seq_X.append(X[i:i+window])\n",
        "        if label_col is not None:\n",
        "            seq_y.append(y_raw[i+window-1 + horizon])\n",
        "    X = np.array(seq_X)\n",
        "    if label_col is not None:\n",
        "        y = np.array(seq_y)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def chronological_split(X, y=None, train=0.7, val=0.15):\n",
        "    n = len(X)\n",
        "    n_train = int(n * train)\n",
        "    n_val = int(n * val)\n",
        "    idx_train = slice(0, n_train)\n",
        "    idx_val = slice(n_train, n_train + n_val)\n",
        "    idx_test = slice(n_train + n_val, n)\n",
        "    if y is None:\n",
        "        return X[idx_train], X[idx_val], X[idx_test]\n",
        "    return X[idx_train], y[idx_train], X[idx_val], y[idx_val], X[idx_test], y[idx_test]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "usecols_common = [\n",
        "    'wired_latency_ms','satellite_latency_ms',\n",
        "    'wired_jitter_ms','satellite_jitter_ms',\n",
        "    'wired_packet_loss_pct','satellite_packet_loss_pct',\n",
        "    'wired_bandwidth_mbps','satellite_bandwidth_mbps',\n",
        "    'wired_quality_cost','satellite_quality_cost'\n",
        "]\n",
        "\n",
        "# A: includes label current_optimal_path\n",
        "A = pd.read_csv(DATASET_A, usecols=lambda c: (c in usecols_common) or (c=='current_optimal_path') or (c=='timestamp'))\n",
        "B = pd.read_csv(DATASET_B, usecols=lambda c: (c in usecols_common) or (c=='timestamp'))\n",
        "\n",
        "# Encode label\n",
        "a_label = A['current_optimal_path'].str.lower().map({'wired':0,'satellite':1}).astype(int)\n",
        "A = A.drop(columns=['current_optimal_path'])\n",
        "\n",
        "print('Shapes -> A:', A.shape, 'B:', B.shape)\n",
        "A.head(), B.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features (numeric only; exclude timestamp)\n",
        "scaler = StandardScaler()\n",
        "numeric_cols = usecols_common  # the numeric feature list you defined earlier\n",
        "\n",
        "A_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(A[numeric_cols]),\n",
        "    columns=numeric_cols\n",
        ")\n",
        "B_scaled = pd.DataFrame(\n",
        "    scaler.transform(B[numeric_cols]),\n",
        "    columns=numeric_cols\n",
        ")\n",
        "\n",
        "# Save scaler\n",
        "import joblib\n",
        "joblib.dump(scaler, ARTIFACTS_DIR / 'scaler.pkl')\n",
        "print('Scaler saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Windowing\n",
        "FEATURES = numeric_cols\n",
        "WINDOW = 12  # ~ 12 time-steps (adjust to your sampling rate)\n",
        "HORIZON = 6  # predict 60s ahead if 10s step\n",
        "\n",
        "XA, ya = create_sequences(pd.concat([A_scaled, a_label.rename('label')], axis=1), FEATURES, 'label', WINDOW, HORIZON)\n",
        "XB, _ = create_sequences(B_scaled, FEATURES, None, WINDOW, 1)\n",
        "\n",
        "print('XA', XA.shape, 'ya', ya.shape, 'XB', XB.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chronological splits\n",
        "XA_tr, ya_tr, XA_val, ya_val, XA_te, ya_te = chronological_split(XA, ya)\n",
        "XB_tr, XB_val, XB_te = chronological_split(XB)\n",
        "\n",
        "len(XA_tr), len(XA_val), len(XA_te), len(XB_tr), len(XB_val), len(XB_te)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predictive model (LSTM classifier)\n",
        "num_features = XA.shape[-1]\n",
        "\n",
        "clf = models.Sequential([\n",
        "    layers.Input(shape=(WINDOW, num_features)),\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "clf.compile(optimizer=optimizers.Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "clf.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train classifier\n",
        "ckpt_clf = callbacks.ModelCheckpoint(filepath=str(ARTIFACTS_DIR / 'predictive_clf.keras'), monitor='val_accuracy', save_best_only=True)\n",
        "early = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "hist_clf = clf.fit(\n",
        "    XA_tr, ya_tr,\n",
        "    validation_data=(XA_val, ya_val),\n",
        "    epochs=20,\n",
        "    batch_size=256,\n",
        "    callbacks=[ckpt_clf, early],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "clf_best = models.load_model(ARTIFACTS_DIR / 'predictive_clf.keras')\n",
        "loss, acc = clf_best.evaluate(XA_te, ya_te, verbose=0)\n",
        "print({'test_loss': float(loss), 'test_accuracy': float(acc)})\n",
        "with open(ARTIFACTS_DIR / 'predictive_metrics.json', 'w') as f:\n",
        "    json.dump({'test_loss': float(loss), 'test_accuracy': float(acc)}, f, indent=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anomaly detector (LSTM Autoencoder)\n",
        "use_device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
        "strategy = tf.distribute.OneDeviceStrategy(device=use_device)\n",
        "print(\"Autoencoder strategy device:\", use_device)\n",
        "with strategy.scope():\n",
        "    enc_in = layers.Input(shape=(WINDOW, num_features))\n",
        "    enc = layers.LSTM(64, return_sequences=True)(enc_in)\n",
        "    enc = layers.LSTM(32)(enc)\n",
        "    lat = layers.RepeatVector(WINDOW)(enc)\n",
        "\n",
        "    dec = layers.LSTM(32, return_sequences=True)(lat)\n",
        "    dec = layers.LSTM(64, return_sequences=True)(dec)\n",
        "    out = layers.TimeDistributed(layers.Dense(num_features))(dec)\n",
        "\n",
        "    autoenc = models.Model(enc_in, out)\n",
        "    autoenc.compile(optimizer=optimizers.Adam(1e-3), loss='mse')\n",
        "autoenc.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train autoencoder on normal-only windows (XB_tr)\n",
        "ckpt_ae = callbacks.ModelCheckpoint(filepath=str(ARTIFACTS_DIR / 'autoencoder.keras'), monitor='val_loss', save_best_only=True)\n",
        "early_ae = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "hist_ae = autoenc.fit(\n",
        "    XB_tr, XB_tr,\n",
        "    validation_data=(XB_val, XB_val),\n",
        "    epochs=20,\n",
        "    batch_size=256,\n",
        "    callbacks=[ckpt_ae, early_ae],\n",
        "    verbose=1\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anomaly scoring: reconstruction error distribution\n",
        "best_ae = models.load_model(ARTIFACTS_DIR / 'autoencoder.keras')\n",
        "\n",
        "recon_tr = np.mean(np.square(XB_tr - best_ae.predict(XB_tr, verbose=0)), axis=(1,2))\n",
        "recon_te = np.mean(np.square(XB_te - best_ae.predict(XB_te, verbose=0)), axis=(1,2))\n",
        "\n",
        "threshold = float(np.percentile(recon_tr, 99))\n",
        "metrics = {\n",
        "    'train_mean_error': float(np.mean(recon_tr)),\n",
        "    'train_99pct_threshold': threshold,\n",
        "    'test_mean_error': float(np.mean(recon_te))\n",
        "}\n",
        "print(metrics)\n",
        "with open(ARTIFACTS_DIR / 'anomaly_metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# Plot distributions\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.kdeplot(recon_tr, label='train')\n",
        "sns.kdeplot(recon_te, label='test')\n",
        "plt.axvline(threshold, color='r', linestyle='--', label='threshold@p99')\n",
        "plt.legend(); plt.title('Reconstruction Error Distributions')\n",
        "plt.savefig(ARTIFACTS_DIR / 'reconstruction_error_distributions.png', dpi=200, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What to expect\n",
        "- `predictive_clf.keras`, `predictive_metrics.json`: classifier and test metrics (accuracy).\n",
        "- `autoencoder.keras`, `anomaly_metrics.json`: autoencoder and reconstruction error stats.\n",
        "- `reconstruction_error_distributions.png`: density plot showing train vs test error and a 99th percentile threshold.\n",
        "\n",
        "You can download artifacts from the right-hand panel (/kaggle/working) in Kaggle.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Package artifacts for download (no changes to previous cells)\n",
        "import os, json, shutil, zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "art_dir = ARTIFACTS_DIR\n",
        "bundle_dir = art_dir / 'bundle'\n",
        "bundle_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Copy produced artifacts if present\n",
        "files_to_copy = [\n",
        "    'predictive_clf.keras',\n",
        "    'autoencoder.keras',\n",
        "    'scaler.pkl',\n",
        "    'predictive_metrics.json',\n",
        "    'anomaly_metrics.json',\n",
        "    'reconstruction_error_distributions.png',\n",
        "]\n",
        "for name in files_to_copy:\n",
        "    src = art_dir / name\n",
        "    if src.exists():\n",
        "        shutil.copy2(src, bundle_dir / name)\n",
        "\n",
        "# Save a manifest with environment + training config\n",
        "manifest = {\n",
        "    'tensorflow': tf.__version__,\n",
        "    'pandas': pd.__version__,\n",
        "    'numpy': np.__version__,\n",
        "    'feature_columns': list(FEATURES),\n",
        "    'window': int(WINDOW),\n",
        "    'horizon': int(HORIZON),\n",
        "}\n",
        "(bundle_dir / 'manifest.json').write_text(json.dumps(manifest, indent=2))\n",
        "\n",
        "# Create zip\n",
        "zip_path = art_dir / 'leo_dual_ai_artifacts.zip'\n",
        "if zip_path.exists():\n",
        "    zip_path.unlink()\n",
        "shutil.make_archive(str(zip_path.with_suffix('')), 'zip', root_dir=bundle_dir)\n",
        "print('Created archive:', zip_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect archive contents (optional)\n",
        "import zipfile\n",
        "with zipfile.ZipFile(ARTIFACTS_DIR / 'leo_dual_ai_artifacts.zip', 'r') as zf:\n",
        "    print('Archive files:')\n",
        "    for n in zf.namelist():\n",
        "        print(' -', n)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
